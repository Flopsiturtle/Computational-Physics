\documentclass[11pt, letterpaper, onecolumn]{article}
 
\usepackage[english]{babel}
\usepackage{soul}
\usepackage{mathtools}
\usepackage[utf8]{inputenc}
\usepackage{graphicx}
\usepackage{float}
\usepackage[german=quotes]{csquotes}
\usepackage{hyperref}
\usepackage{fancyhdr}
\usepackage{gensymb}
\usepackage{units}
\usepackage{siunitx}
\usepackage{hhline}
\usepackage{color}
\usepackage{titling}
\usepackage[normalem]{ulem}
\usepackage[margin=2.5cm]{geometry}
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{amsfonts}
\usepackage{pgfplots}
\usepackage{array}
\usepackage{makecell}
\usepackage{subfigure}
\usepackage{lipsum}
\usepackage{url}
\usepackage{relsize}
\usepackage{braket}

\newgeometry{a4paper, left=20mm, right=20mm, top=30mm, bottom=30mm}
\definecolor{pantone294}{cmyk}{1,0.6,0,0.2}
\setlength{\columnsep}{6mm} 

\title{Project 2: Ising model} 
\author{Florian Telleis / Florian Hollants / Mickey Wilke}
\date{\today}

\pagestyle{fancy}
\lfoot{Humboldt-Universität zu Berlin}
\rfoot{Project 1.1} 



\begin{document}
	
	\newgeometry{left=14mm, right=13.5mm, top=13.5mm, bottom=30mm}
	\begin{titlepage}
		\thispagestyle{empty}
		\begin{figure}
			
		\end{figure}
		\vspace*{-43mm}\hspace{-6mm}\textbf{\textcolor{pantone294}{\large{Mathematisch-Naturwissenschaftliche Fakultät}}}\\\\\\\\\\
		\textcolor{pantone294}{Institut für Physik}\\
		\vspace{30mm}
		\begin{center}
			\textcolor{pantone294}{\huge{Computational Physics II}}\\\vspace*{7mm}
			\textcolor{pantone294}{\huge{\textbf{\thetitle}}}\\\vspace*{10mm}
			\textcolor{pantone294}{\theauthor}\\\vspace*{10mm}
			\textcolor{pantone294}{\thedate}\\\vspace*{20mm}
			\begin{tabular}{ll}
				\textbf{Work Group:} & Albert Einstein	 \\ \\
				\textbf{Students:} & Florian Telleis (612716) \\
									& Florian Hollants (648689)\\
									& Mickey  Wilke (642815)\\ \\
				\textbf{Submitted:} & 15.02.2025 \\ \\				
			\end{tabular}
		\end{center}
	\end{titlepage}
	\makeatother
	\restoregeometry
		
		\newpage
	
	
	
	
	
	
    \tableofcontents
    \vspace{1cm}
    
    
    
    
    
    \section{How the code works}
    \underline{Generating Markov Chains and state evolution}
    \\
    \\
    \noindent
    To generate a Markov chain, one has to simply run the "variables.cpp" file. You will be asked to input the number of dimensions, the number of points in each dimension, the length of the Markov chain, as well as the values for $\beta$ and $b$. Lastly, you will have to input the value of the seed used to generate the random numbers and weather or not to start in a cold state (1 for cold state and 0 for hot state). The values of the magnetization and the energy will be stored in the "Results" folder in the files called "MagnetizationHistory.csv" and "EnergyHistory.csv", respectively. This will also automatically save the state after each step of the markov chain in the file "stateEvolution.csv". This file will be used in the file "stateEvolution.py" which will visualize the system for each step in the chain, when executed. This visualization only works in 2D and for $N=100$, but in the "Results" folder are gifs for different Markov chains.
   
    \noindent
    \underline{Calculation of the mean energy and magnetisation}
    \\
    \\
    \noindent
    The mean energy, magnetisation and the corresponding errors may be calculated with the file \textbf{calculate$\_$means.py} and by using \textbf{EnergyReplica.csv} from the folder \textbf{Results}.\\
    The corresponding path to the \textbf{EnergyReplica.csv} file needs to be entered into the \textbf{pd.read$\_$csv()} function. Using the \textbf{changing$\_$thermalisation$\_$cutoff.py} file the corrsponding plot \ref{thermcutoff} can be reproduced. The path to the \textbf{EnergyReplica.csv} and \textbf{MagnetizationReplica.csv} file need to be provided in the functions.\\
    
    \noindent
    \underline{Numerical analysis of the statistical errors}
    \\
    \\
    \noindent
    In order to replicate the diagrams which show the behavior of the statistical errors you need to use the files \textbf{change$\_$configuration$\_$number.py} and \textbf{change$\_$replica$\_$number.py} for the plots regarding the number of configurations and the number of replicas respectively. In this file there are two spots where paths to the files \textbf{EnergyReplica.csv} and \textbf{MagnetizationReplica.csv} need to be provided once again in the \textbf{pd.read$\_$csv()} function. \\


    \noindent
    \underline{Probability distributions}
    \\
    \\
    \noindent
    To plot the probability distribution, you must run the file \textbf{prob$\_$distr.py}. The input of the functions used for plotting the histograms can be changed regarding the input data (.csv file) and the parameters of the bootstrap-method. Finally, at the end of the file there is the code for visualizing our original replicas - however, this is not being plotted when you run the file, you would need to delete a "$\#$" at the end of the code.

    \noindent
    \underline{Temperature dependency of the densities}
    \\
    \\
    \noindent
    !!!part of flo about how the the data in Results2 can be replicated!!!\\
    The file \textbf{temperature$\_$dependency.py} generates the plots, figure \ref{tempdepend}, for the temperature dependencies. Just as before there are two spots where the paths need to be provided. In order for the code to work properly all the required files should be stored in one folder which we sent you as \textbf{Results2}. If you keep the data in a folder named \textbf{Results2} you only need to change the part of the path before that.\\
    Figure \ref{tempdependfixed} with some datapoints removed may be generated using  \textbf{temperature$\_$dependency$\_$fixed.py} with the folders \textbf{Results2} for the energy and \textbf{Results4} for the magnetisation.\\
    The last two figures \ref{energycritical} and \ref{magcritical} are created with \textbf{energy$\_$replica$\_$critical.py} and \textbf{mag$\_$replica$\_$critical.py} using the data from the folder \textbf{Results3}.



    \pagebreak
    \section{Workflow}
    \underline{Calculation of the mean energy and magnetisation}
    \\
    \\
    \noindent
	The first thing we had to think about was how to represent the D-dimensional grid. Coming from python, where one can easily make ndarrays, we had to use a new approach, in which we save all the spins in one long 1-D array, and associate each index with a position on the grid. To translate from index to position and vice versa we wrote the "indexToPosition" and "positionToIndex" functions using periodic boundary conditions. Next we set the state vector in either the hot or cold condition. For the hot condition, we decided to use the "default\_random\_engine" class from the "<random>" module. Next we wrote the functions to calculate the energy and magnetization of the system and wrote the metropolis algorithm according to the lecture notes. In the hopes of increasing the speed of the algorithm, we then implemented a function that calculates the neighbors of each spin, so that the hamiltonian function doesn't have to do that every time it is called. This function is only called once in the beginning and stores all the indices of the neighbors on the grid. The second thing we tried, was saving each spin as a bit, instead of saving it as an integer. Since each spin can only be in one of two states, one bit is all we should need to store each of them. In order to do that, we wrote functions to "translate" between the char vector, and the value of the spins at each index. Lastly we wrote the "generateHistory" and "appendColumn" functions to make the history plots and save the data in their corresponding .csv files.
    \noindent
    \underline{Calculation of the mean energy and magnetisation}
    \\
    \\
    \noindent
    Once the data for 500 replicas was generated we could begin checking the data for thermalisation. We looked at all the replicas and determined the thermalisation to take place at around $N_{th} = 200$. After this we implemented the given way of calculating the means and statistical errors from the lecture. By changing $N_{th}$ we could also confirm that our choice was appropriate since the calculated quantities only changed minimally and stayed in each others error bounds. This was then also confirmed by systematically changing the thermalisation as can be seen in figure \ref{thermcutoff}.\\

    \noindent
    \underline{Numerical analysis of the statistical errors}
    \\
    \\
    \noindent
    We chose to vary the number of configurations and replica by modifying one data set of 500 replicas. The statistical error is calculated  in the same manner as for the means. When looking at the the behavior of the error, see figures \ref{varyconfig} and \ref{varyreplica}, it is evident that it decreases with growing number of configurations and replicas. It is also evident that there is some kind of functional dependency which we confirmed by fitting the data to the theoretically predicted function. In theory one would expect the following proportionalities for statistical quantities $X(n)$, $\bar{X}^r$, $\bar{\bar{X}}$ and $\textit{err}(\Bar{\Bar{X}})$:
    \begin{align*}
       &\bar{X}^r = \frac{1}{N}\underbrace{\sum_{n=1}^N X(n)}_{\sim N} \sim N^0, R^0\\
       &\bar{\bar{X}} = \frac{1}{R}\underbrace{\sum_{r=1}^R \bar{X}^r}_{\sim R} \sim N^0, R^0\\
       &\textit{err}(\Bar{X}^r) = \sqrt{\frac{1}{N(N-1)} \underbrace{\sum_{n=1}^N (X(n) - \bar{X}^r)^2}_{\sim N}}  \sim \sqrt{\frac{1}{N-1}} \sim  \sqrt{\frac{1}{N}}\\
       &\textit{err}(\Bar{\Bar{X}}) = \sqrt{\frac{1}{R(R-1)} \underbrace{\sum_{r=1}^R (\bar{X}^r - \bar{\bar{X}})^2}_{\sim R}} \sim  \sqrt{\frac{1}{R}}\\
       &\textit{err}(\Bar{\Bar{X}}) =\frac{1}{R} \sqrt{\sum_{r=1}^R \underbrace{(\textit{err}(\Bar{X}^r))^2}_{\sim \frac{1}{N}}}  \sim  \sqrt{\frac{1}{N}}         \\
       & \implies R, N \sim \frac{1}{(\textit{err}(\Bar{\Bar{X}}))^2}\\
    \end{align*}

    \noindent
    \underline{Probability distribution} \\
    A simple probability distribution for our calculated data of 500 mean values (500 replicas with one mean per replica, each for magnetization and energy) can easily be calculated with numpy´s histogram module and then be plotted with matplotlib´s bar module. These histograms for our original 500 replicas can be seen in the appendix. For this, we also tested the expectation of a Gaussian distribution for our histograms via fitting a Gaussian curve to the data and then displaying both overlayed, which worked very well. (This also showed the importance of the further down explained mean error as a standard deviation.)\
    Moreover, as we want to implement specific errors for each bar into our distribution, we can fundamentally choose between two procedures: \\
    1. We can compute a lot more replicas for the same given parameters to then calculate the overall mean and statistical error (with given equations in lecture) for each bar of the histogram. This would be an exact method. However, in order to not have to compute hundreds and thousands of more replicas we chose a resampling procedure. \\
    2. The bootstrap-method can be used to estimate a real sampling distribution by just resampling our given data. We implemented the method in the following way: first, we take a certain number (function input "size$\_$small$\_$samples") of elements from the given array and append these values into a new array until we reach the original length of the given data; in our case 500. This will be done a specific number of times, e.g. N-times (function input "numb$\_$samples"). Also using the original data, at the end we get (N+1) "independent" arrays of same size; just as we would have computed 500 replicas and their means (N+1)-times explicitly. This procedure is of course an approximation, but we are also working with statistical changes from one replica to another, which underlines the bootstrap-method being used. 
    \\
    As already explained for method 1, we can split our newly computed arrays of replica-means into specific bars of the final histogram (function input "numb$\_$bars"). For each bar, we calculate the overall mean with its statistical error, as explained in the lecture. These values can then be used for the final histogram with error bars. The histogram plot was afterwards refurbished for better visibility and evaluation of the distributions.
    \\
    \\
    An important note: as expected for statistical processes, the outcome of our bootstrap-method is highly dependent on the chosen parameters of sampling number and chosen random seed. These parameters were tested extensively with hindsight on the error of the overall mean via the bootstrap method being criterion for statistical $\sigma$-deviation. The parameters used for our final calculations via bootstrap-method are: random.seed=5, numb$\_$bars=50, numb$\_$boot$\_$samples=100, size$\_$small$\_$samples=5.


    \noindent
    \underline{Temperature dependency of the densities}
    \\
    \\
    \noindent
    !!!worklfow of flo regarding how the data was created!!!\\
    With the data sets for the different values of $\frac{1}{\beta}$ and $B$ we could again calculate the means and statistical errors for each set and plot the densities $\frac{<E>}{100^2}$ and $\frac{<M>}{100^2}$ in dependence on the temperature $\frac{1}{\beta}$ for different magnetic fields $B$, see figure \ref{tempdepend}. As Prof. Patella pointed out some datapoints in this figure for small temperatures seem to exhibit unphysical behavior for the magnetisation. The magnetisation should not decrease after the phase transition into the magentised state. By reviewing the history plots of the data points, see figure \ref{magcritical}, it was evident that the issue lies with the thermalization point. After the phase transition the point of thermalization increases,  requiring longer chains. This becomes undoable, due to computation time, very quickly for lower temperatures and thus can be considered as a limit of our numerical method. Figure \ref{tempdependfixed} shows the same graph but only using replica sets where all replicas are thermalized.    

    \pagebreak

    \section{Results}
    \underline{Calculation of the mean energy and magnetisation}
    \\
    \\
    \noindent
   The calculation of the the means and statistical errors leads to the following values:
   \begin{align*}
       &<E> = \qty{-6,2360 +- 0,0007}{}\cdot 10^3\\
       &<M> = \qty{7,181 +- 0,003}{}\cdot 10^3\\
   \end{align*}
   This is in accordance with the values which we were supposed to calculate.\\
   We also investigated the the dependence of the means and errors on the thermalization cut off as can be seen in figure \ref{thermcutoff}. There we can see that the means converge towards a value which is characteristic for the process of thermalization. From the two lower plots for the errors we can also see why it is important to choose an appropriate cut-off value for $N_{th}$ which maximizes the number of available configurations. The error grows when we cut off at a higher value of $N$ which is to be expected since a higher cut-off results in less available configurations for the calculation of the means. We further investigate the behaviour of the statistical errors in the next section.
    \begin{figure} [H] 
	\begin{center}	
	\includegraphics[width=14cm]{varying_thermalisation_cutoff_plot.png}
	\caption{Top two diagrams: Behavior of the mean energy and magnetisation depending on the thermalization cut off. Thermalization sets in after the values converge. For even larger cut off configurations the error on the means increase. For further clarity the bottom two diagrams explicitly show the error growing when we increase the cut-off configuration after thermalization.} \label{thermcutoff}
	\end{center}
    \end{figure}


    \noindent
    \underline{Numerical analysis of the statistical errors}
    \\
    \\
    \noindent
    First off we can see in both diagrams \ref{varyreplica} and \ref{varyconfig} that the statistical error decreases for increasing configuration number $N$ and number of replicas $R$. When varying $R$ we get some fluctuations in the error for low replica number. This is to be expected though since the error itself is a statistical variable which is subject to a statistical error. As described in the workflow we expect a proportionality of the form $ R, N \sim 1/(\textit{err}(\Bar{\Bar{X}}))^2$ with $\Bar{\Bar{X}} \in \{<E>, <M>\}$. In order to confirm that we plot $R, N$ in both plots against $1/(\textit{err}(\Bar{\Bar{X}}))^2$ and see a linear form. By doing a linear regression(least squares) we fit the linearised error to a function $f = mx +n$ with the following results:
    \begin{align*}
    \begin{tabular}{l|c|c}
        Varied quantity &  energy error  &  magnetisation error \\
        \hline
        N & f = \qty{3,259 +- 0.007}{} \cdot 10^{-3}\cdot x - \qty{5 +- 3}{}\cdot 10^{-3} & f=  \qty{2,854 +- 0.005}{} \cdot 10^{-4}\cdot x - \qty{5 +- 2}{}\cdot 10^{-4} \\
        \hline
        R & f = \qty{4,983 +- 0.009}{} \cdot 10^{-3}\cdot x - \qty{5 +- 25}{}\cdot 10^{-4}   &  f=\qty{4,476 +- 0,007}{} \cdot 10^{-4}\cdot x - \qty{2,5688 +- 0.0002}{}\cdot 10^{-5}   \\
    \end{tabular}
    \end{align*}
    The fits strongly support the thesis that $\textit{err}(\Bar{\Bar{M}})$ and $\textit{err}(\Bar{\Bar{E}})$ scale with $1/\sqrt{N}, 1/\sqrt{R}$.
    
    \begin{figure} [H] 
	\begin{center}	
	\includegraphics[width=15cm]{vary_replica_number_plot_final.png}
	\caption{The upper two diagrams show the decreasing statistical error against the number of replicas for the mean energy(left) and magnetisation(right) error. The two diagrams below show $1/\textit{err}(<E>)^2$ and  $1/\textit{err}(<M>)^2$ against the number of replicas and the corresponding linear fits.} \label{varyreplica}
	\end{center}
    \end{figure}



    
    \begin{figure} [H] 
	\begin{center}	
	\includegraphics[width=15cm]{vary_config_number_plot_final.png}
	\caption{The upper two diagrams show the decreasing statistical error against the number of configurations for the mean energy(left) and magnetisation(right) error. The two diagrams below show $1/\textit{err}(<E>)^2$ and  $1/\textit{err}(<M>)^2$ against the number of configurations and the corresponding linear fits. } \label{varyconfig}
	\end{center}
    \end{figure}



\subsection{Probability distribution}
	We now evaluate the probability distributions of magnetization and energy for our original 500 replicas. The procedure used for calculating our data and resampling sets was already explained in the workflow. Thus, both of the histograms can be seen in the figures \ref{fig:histo_final_magn} and \ref{fig:histo_final_energy} below. All parameters for the bootstrap-method and the plotting itself, except the x-axis and therefore x-size of the bars, were kept the same for both figures.
	\begin{figure} [h] 
	\begin{center}
	\includegraphics[width=17cm]{"magn_histo_final.png"}
\caption{Magnetization histogram via bootstrap-method. With statistical error bars, mean of the original- and bootstrap-distribution, respective statistical error added as $\Delta x$ bars (view values in figure).}	\label{fig:histo_final_magn}
	\end{center}
	\end{figure}
	\begin{figure} [h] 
	\begin{center}
	\includegraphics[width=17cm]{"energy_histo_final.png"}
\caption{Energy histogram via bootstrap-method. With statistical error bars, mean of the original- and bootstrap-distribution, respective statistical error added as $\Delta x$ bars (view values in figure).}	\label{fig:histo_final_energy}
	\end{center}
	\end{figure}
	\\
	\\
	One can observe a relatively good Gaussian distribution for both histograms, as expected for a statistical deviation of replicas which are statistically independent of one and each other (viable for calculating replicas and bootstrapping them). Both the beforehand and via bootstrap calculated mean are nearly identical (maximum $\pm0.2$ for magnetization). Whereas the bootstrap errors are around 50-times larger, stemming from the statistical resampling nature of the bootstrap-method. Still, just from looking at both plots, one could argue, that the errors of the bootstrap mean are approximately in a $1\sigma$-deviation. Which is expected for the Gaussian distribution around a 'true' mean, as explained at the beginning of the workflow section "probability distribution". \\
	Furthermore, the energy means are more densely distributed around the overall mean than the magnetization means, also visible in different sizes of errors. This can also be seen in figure \ref{fig:app1} in the appendix, where the same energy histogram is plotted with the x-axis being in the same size limits as for the magnetization.
	Finally, it is important to discuss the outcome of the bootstrap-method in regards to the original replica set. By looking at the histograms of the original replicas, one notices a distinct similarity to the bootstrap histogram. For example the high "mean" peak for the energy histograms. This is of course obvious, as we are just resampling our data, but it has to be noted nonetheless. Therefore, for additional more careful analysis of the task, it would be wiser to explicitly calculate lots of replicas which then can be used for a probability distribution with errors. As for our task, the method we used worked well enough for interpretation.



    \noindent
    \underline{Temperature dependency of the densities}
    \\
    \\
    \noindent
	The expected curve clearly shows the phase transition, between ferro- and paramagnet. We expected the temperature curve to go to $m=1$ relatively quickly, but our simulation seems to show something different. The average magnetization is clearly non-zero, but it does not go to one, but somewhere in between, depending on the external magnetic field. The reason for that is the formation of domain walls, as can be seen in the gif for $\beta=5.0$. Depending on the starting configuration, it can occasionally happen, that domain walls form, that are extremely unlikely to be broken again, especially when $\beta$ is much bigger than $b$. A spin at the boundary of the domain wall has three neighbors pointing in the same direction, and one in the opposite, so we can calculate $\Delta H$ to be
\begin{align*}
    \Delta H = 4\beta-2b
\end{align*}
Therefore the probability of flipping a particular spin in each iteration in the case for $\beta = 5.0$ and $b=0.01$
\begin{align*}
    P(s'\rightarrow s)=e^{-\Delta H}\approx 2\cdot10^{-9}
\end{align*}
in our case the length of the domain wall boundary is $L=2N=200$ and so the probability for any spin to flip in one iteration is approximately $4\cdot10^{-7}$. The only redeeming factor is, that once the domain wall boundary is broken, it will inevitably become one "layer" smaller, since the spin right before it in the loop of the metropolis method, will have two neighbors pointing in the same direction and two in the opposite. Therefore $\Delta H<0\implies P(s'\rightarrow s)=1$. Still, we would have to increase the length of the markov chain to approximately $10^8$ steps to see the domain wall disappear entirely. Another case can also be seen, where the domain wall encompasses the whole crystal. In that case its not really a domain wall anymore, but every spin is pointing opposite to the magnetic field. In this case the probability to flip any one spin is given by
\begin{align*}
    N^DP(s'\rightarrow s)=10^4\cdot e^{-(8\beta-2b)}\approx 4\cdot10^{-14}
\end{align*}
only that now, the size of the domain wall isn't reliably reduced anymore, since we go from a spin having 4 neighbors pointing in the same direction to three which we covered in the example before, and is still extremely unlikely to flip.
     \begin{figure} [H] 
	\begin{center}	
	\includegraphics[width=15cm]{temperature_dependence_plot.png}
	\caption{The left plots show the temperature dependency of the energy density for varying values of $B, \beta$. The right plots show the magnetisation-density. The phase transition from unmagnetised to magnetised can be seen clearly for decreasing temperatures. The points left of the phase transition for the magnetisation-density are unphysical due to the replicas not being thermalized. This is fixed in \ref{tempdependfixed}.} \label{tempdepend}
	\end{center}
    \end{figure}



 \begin{figure} [H] 
	\begin{center}	
	\includegraphics[width=15cm]{temperature_dependency_fixed_plot.png}
	\caption{Same plots as in figure \ref{tempdepend} with the unphysical points removed from the magentisation density.} \label{tempdependfixed}
	\end{center}
    \end{figure}

     \begin{figure} [H] 
	\begin{center}	
	\includegraphics[width=15cm]{mag_replicas_critical_plot.png}
	\caption{Magnetisation history plots for all replicas with longer Markov chains for chosen values of $B, \beta$ which did not thermalize in figure \ref{magcritical}. As can be seen the chains do thermalize for longer chains.} \label{magcritical}
	\end{center}
    \end{figure}

     \begin{figure} [H] 
	\begin{center}	
	\includegraphics[width=15cm]{energy_replica_critical_plot.png}
	\caption{Energy history plots for all replicas with longer Markov chains for chosen values of $B, \beta$ as comparison to figure \ref{magcritical}. All the replicas thermalize very fast in stark contrast to figure \ref{magcritical}.} \label{energycritical}
	\end{center}
    \end{figure}




\newpage

	
\section{Appendix} \label{sec:appendix}

	\begin{figure} [h] 
	\begin{center}
	\includegraphics[width=17cm]{"energy-mag_histo_final.png"}
\caption{To visualize the narrower distribution of the energy in regards to magnetization: same energy histogram via bootstrap-method as seen before, now with the x-axis being in the same size limits as for the magnetization. With statistical error bars, mean of the original- and bootstrap-distribution, respective statistical error added as $\Delta x$ bars (view values in figure).}	\label{fig:app1}
	\end{center}
	\end{figure}


\begin{figure} [h] 
	\begin{center}
	\includegraphics[width=11cm]{"orig_magn.png"}
\caption{Magnetization: original data histogram and Gaussian "fit"}	\label{fig:app2}
	\end{center}
	\end{figure}

	\begin{figure} [h] 
	\begin{center}
	\includegraphics[width=11cm]{"orig_energy.png"}
\caption{Energy: original data histogram and Gaussian "fit"}	\label{fig:app3}
	\end{center}
	\end{figure}	
	





	
\end{document}
